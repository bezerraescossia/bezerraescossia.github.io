<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h4>Twitter Pipeline</h4> <p>Nesse projeto, iremos extrair dados gerados no Twitter para que equipes de dados (cientistas de dados, analistas de dados, de negócio, de BI e outros) possam explorar essas informações e tirar <em>insights</em> a partir delas.</p> <p>Ao contrário de um processo de ETL (extração, transformação e carregamento) pontual, nesse <em>pipeline</em> utilizaremos um processamento em <em>batch</em> dessas informações. Para isso ser possível, precisaremos de um orquestrador de tarefas (Apache Airflow).</p> <p>Além disso, por estarmos inserido num ambiente que possa gerar um grande volume de dados (big data), como é o caso das redes sociais, utilizaremos, também, o processamento distribuído por meio do Apache Spark.</p> <p>Portanto, de forma resumida, o escopo do nosso projeto consiste em:</p> <p>- Extrair dados do Twitter através da sua API de desenvolvedor.<br>- Transformar os dados brutos em informações mais “limpas”.<br>- Armazenar os dados extraídos em um datalake, categorizado quanto a sua “pureza” (bronze, silver, gold).</p> <p>Lembrando que essas tarefas são gerenciadas pelo Apache Airflow e os dados são manipuladas pelo Apache Spark para uma melhor eficiência no processamento.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/819/1*PmQcH__6MZgqxvGMRJvMAw.png"></figure> <p>A título de exemplificação, nesse projeto estamos coletando todos os tweets relacionados ao perfil @ufrnbr, que pertence a Universidade Federal do Rio Grande do Norte.</p> <h3>Preparando o ambiente</h3> <p>Como dito anteriormente, iremos utilizar a API disponibilizada pelo Twitter de forma gratuita. Portanto, o primeiro passo desse projeto é criar a conta na <a href="https://developer.twitter.com/en" rel="external nofollow noopener" target="_blank">plataforma</a> e aplicar-se ao acesso dessa ferramenta. Visto que o foco dessa publicação é a construção do <em>pipeline</em>, deixo o <a href="https://dev.to/sumedhpatkar/beginners-guide-how-to-apply-for-a-twitter-developer-account-1kh7" rel="external nofollow noopener" target="_blank">link do post</a> para mais informações sobre esse processo.</p> <p>Com a conta <em>Twitter Developer</em> devidamente aprovada e em funcionamento, podemos então preparar o ambiente de desenvolvimento.</p> <p>O próximo passo é instalar o <em>Apache Airflow</em>. Para a instalação e utilização dessa ferramenta em máquina local, há dois caminhos possíveis de se trabalhar:</p> <ul> <li>Em um contêiner Docker.</li> <li>Em um ambiente virtual.</li> </ul> <p>Nesse projeto, o Apache Airflow foi instalado num ambiente virtual. Portanto, inicialmente devemos criar uma pasta de trabalho, configurar a versão do Python a ser utilizada e criar o ambiente virtual.</p> <p>Para o gerenciamento da versão do Python foi utilizado o <em>pyenv</em> e no gerenciamento do ambiente virtual, o <em>virtualenv</em>.</p> <pre>mkdir twitter_project<br>cd twitter_project<br>pyenv local 3.7.12<br>virtualenv .venv<br>source .venv/bin/activate</pre> <p>Além disso, é necessário criar uma variável de ambiente no shell que aponta para a pasta onde o <em>Airflow</em> será instalado:</p> <pre>export AIRFLOW_HOME=$(pwd)/airflow</pre> <p>Lembre sempre de manter esta variável ativa, caso contrário o Airflow será criado na pasta raiz do usuário. Feito isso, instalaremos o Airflow no nosso ambiente, aqui estamos usando a versão 2.1.4 com o python 3.7:</p> <pre>pip install "apache-airflow[celery]==2.1.4" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.1.4/constraints-3.7.txt"</pre> <p>Após a instalação, podemos, enfim, iniciar o banco de dados da ferramenta, criar o usuário e senha de acesso, iniciar a <em>User Interface</em> (UI) e o agendador de tarefas (<em>scheduler</em>) da ferramenta.</p> <pre>airflow db init<br>aiirflow users create -u admin -f FIRST_NAME -l LAST_NAME -r Admin -e admin@example.org<br>airflow scheduler<br>airflow webserver</pre> <p>Por fim, é possível acessar a UI do Airflow através do navegador no endereço localhost:8080.</p> <h3>Conectando o Airflow ao Twitter</h3> <p>O Airflow permite interagir com fontes de dados e ferramentas externas como a API do Twitter ou também com bancos de dados, servidores SFTP ou serviços na nuvem. Para cada conexão, é necessário um local seguro para armazenar os dados dessa conexão, que pode ser um usuário e senha para um banco de dados, ou, no nosso caso, um token para a API.</p> <p>Para acessar esse local seguro, utilizamos a UI do Airflow e navegamos pelo caminho “Admin &gt; Connections”. Nessa página, são listadas todas as conexões padrões da ferramenta. Em nosso caso, a conexão com o Twitter não existe quando iniciamos um Airflow db. Portanto, é necessária a sua criação:</p> <ul> <li>Conn id: twitter_default</li> <li>Conn Type: HTTP</li> <li>Host: <a href="https://api.twitter.com/" rel="external nofollow noopener" target="_blank">https://api.twitter.com</a> </li> <li>Extra: {‘Authorization’: ‘Bearer SEU_TOKEN’}</li> </ul> <p>Dessa forma, a conexão com a api do Twitter está estabelecida e podemos passar a nossa atenção para os scripts de extração.</p> <h3>Criando o script de gancho com o Twitter</h3> <p>Nesse contexto, é oportuno comentar que o Airflow funciona sob o conceito de <em>DAGs</em> (Directed Acyclic Graph) — exploraremos mais a frente sobre esse conceito — e, por sua vez, elas atuam por meio de operadores. O script do operador que realizará a extração dos dados do Twitter necessita reconhecer a conexão com a API Twitter Developer. E, essa ponte, é realizada pelos <em>hooks</em> (ganchos).</p> <p>Portanto, de forma resumida, nosso produto final desejado é um arquivo que controla a DAG, entretanto antes precisamos de um operador e que, por sua vez, precisa de um gancho (ficará mais claro ao final do projeto).</p> <blockquote> <em>Observe o Script do gancho no </em><a href="https://github.com/bezerraescossia/twitter_project/blob/main/airflow/plugins/hooks/twitter_hook.py" rel="external nofollow noopener" target="_blank"><em>link</em></a> </blockquote> <h3>Criando operadores conectados aos ganchos</h3> <p>Como visto anteriormente, as DAGs é um conjunto de passos com início e fim, no qual podem ser executados em sequência ou em paralelo. No Airflow esses passos são provisionados por operadores.</p> <p>Os operadores possuem três características fundamentais:</p> <ul> <li> <strong>Idempotência</strong>, ou seja, independente da quantidade de vezes que você realize uma tarefa, você terá sempre o mesmo resultado.</li> <li>A segunda característica é o <strong>Isolamento</strong>, isso significa que as tarefas não vão compartilhar de uma mesma estrutura, ou seja, funcionam de forma isolada.</li> <li>E por fim, os operadores devem manter sua <strong>atomicidade</strong>. Dessa forma, cada operador deve ter uma tarefa única e indivisível. Por exemplo, se pensarmos em um processo ETL, há três processos atômicos (extração, transformação e carregamento).</li> </ul> <blockquote> <em>Veja o Script do operador no </em><a href="https://github.com/bezerraescossia/twitter_project/blob/main/airflow/plugins/operators/twitter_operator.py" rel="external nofollow noopener" target="_blank"><em>link</em></a> </blockquote> <h3>Transformando os dados com Spark</h3> <p>O Spark é uma ferramenta que permite o processamento distribuído, possibilitando mais agilidade em projetos de big data. Para sua instalação, deixo aqui o link do post.</p> <p>Como abordado na contextualização, os dados devem ser segregados em ouro, prata e bronze. As informações até então extraídas do Twitter estão em seu modo bruto, com todas as informações possíveis em formato JSON. Por outro lado, os arquivos prata e ouro do datalake são dados que já possuem um destino específico e por isso são dados já melhor preparados.</p> <p>Para a instalação do Spark, adicionamos, inicialmente, ao nosso ambiente virtual o <strong>pyspark</strong>, que é a “versão” python do Spark. Essa instalação pode ser feita através do pip.</p> <pre>pip install pyspark</pre> <p>Agora, é necessário instalar os arquivos disponibilizados na <a href="https://spark.apache.org/downloads.html" rel="external nofollow noopener" target="_blank">página de downloads</a> do Spark</p> <p>Para testar se o Spark está funcionando de forma correta, teste o exemplo do cálculo do pi, disponibilizado pela própria aplicação. Para isso, acesse a pasta do spark e execute dentro da pasta:</p> <pre>./bin/spark-submit examples/src/main/python/pi.py 10</pre> <p>Com o Spark funcionando corretamente, podemos seguir em nosso projeto!</p> <blockquote> <em>Visualize nos links as transformações aplicadas aos dados brutos na formação dos dados </em><a href="https://github.com/bezerraescossia/twitter_project/blob/main/spark/transformation.py" rel="external nofollow noopener" target="_blank"><em>prata</em></a><em> e </em><a href="https://github.com/bezerraescossia/twitter_project/blob/main/spark/insight_tweet.py" rel="external nofollow noopener" target="_blank"><em>ouro</em></a> </blockquote> <h3>Construção do arquivo DAG</h3> <p>Como visto, anteriormente, o arquivo DAG é que orquestrará os operadores, executando-os.</p> <p>No script python destinado ao DAG, iniciamos declarando constantes e depois com a classe DAG disponibilizado pelo airflow instanciamos os operador do Twitter e do Spark. Por fim, declara-se a sequência das tarefas.</p> <blockquote> <em>Observe o script do DAG no </em><a href="https://github.com/bezerraescossia/twitter_project/blob/main/airflow/dags/twitter_dag.py" rel="external nofollow noopener" target="_blank"><em>link</em></a> </blockquote> <h3>Próximos Passos</h3> <p>Feito os passos anteriores, o projeto está pronto para funcionamento. Para por em produção, podemos iniciar o scheduler e ativar nossa DAG pela linha de comando ou através da UI do Airflow.</p> <p>Para replicar esse projeto, observe bem a estrutura e disposição das pastas e arquivos através do <a href="https://github.com/bezerraescossia/twitter_project" rel="external nofollow noopener" target="_blank">Github</a>.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=97453bae43de" width="1" height="1" alt=""></p> </body></html>