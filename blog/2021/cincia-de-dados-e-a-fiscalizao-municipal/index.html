<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3>Querido Diário e Ciência de Dados na Fiscalização Pública</h3> <h4>Consultando Diários Oficiais na busca de gastos do dinheiro público com remédios não eficazes no combate ao Covid-19</h4> <p>Aqui você encontrará uma pesquisa, sugerida pelo <a href="https://github.com/leobezerra?tab=repositories" rel="external nofollow noopener" target="_blank">Professor Leonardo Bezerra</a> da disciplina “IMD1151 — Ciência de Dados”, matéria integrante de meu curso de Ciência de Dados na Universidade Federal do Rio Grande do Norte.</p> <p>Autores desse trabalho: Joseane Palhares, Rafael Bezerra, Wagner Gama</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/343/1*CD5QxBMJJ2Lv9ikqWKfPeQ.png"></figure> <h3>Contextualização</h3> <p>De acordo com o próprio site, a Open Knowledge Brasil (OKBR), também chamada de Rede pelo Conhecimento Livre, é uma Organização da Sociedade Civil (OSC) sem fins lucrativos e apartidária, regida por estatuto. A principal missão da OKBR é desenvolver ferramentas cívicas, projetos, análises de políticas públicas, jornalismo de dados e promover o conhecimento livre nos diversos campos da sociedade. Na esfera política, a organização busca tornar a relação entre governo e sociedade mais próxima e transparente.</p> <p>Em resumo, a OKBR é uma rede que desenvolve projetos, visando construir uma sociedade mais transparente e inovadora. Nessa perspectiva, um projeto da organização que ganhou grande notoriedade na mídia foi a Operação Serenata de Amor, que é um projeto aberto e colaborativo que utiliza a ciência de dados para fiscalizar os reembolsos efetuados pela Cota para o Exercício da Atividade Parlamentar (CEAP), verba que custeia alimentação, transporte, hospedagem e até despesas com cultura e assinaturas de TV dos deputados federais.</p> <p>Com o sucesso dessa iniciativa, surgiu a demanda de fiscalizações que transbordem a fronteira de Brasília, que representa apenas uma fatia do orçamento público. Entretanto, ao deparar-se com as esferas municipais, infelizmente, não há muitos dados disponíveis e sem dados não é possível a fiscalização, e é nesse cenário que surge o projeto Querido Diário.</p> <p>Sendo assim, o Querido Diário objetiva realizar, na esfera municipal, o mesmo que o projeto Serenata de Amor fez na esfera federal. Entretanto, atualmente, os diários oficiais são as principais fontes de informação sobre as prefeituras, encontrando-se, na maioria das vezes, em um formato não estruturado, o que dificulta a exploração das informações. Nesse cenário, o projeto Querido Diário aparece justamente para “libertar” e centralizar essas informações e torná-las mais acessíveis para fiscalização.</p> <p>O funcionamento da aplicação, de forma resumida, é baseado na realização de <em>scrapings</em> nos sites oficiais das prefeituras e um posterior tratamento de texto.</p> <p>Neste trabalho, as ferramentas do projeto Querido Diário serão utilizadas para auxiliar na fiscalização da conduta das capitais brasileiras, e outras grandes cidades, em relação a utilização de remédios não eficazes no combate ao covid-19.</p> <h3>Coleta de Dados</h3> <p>O primeiro passo do projeto foi a obtenção dos diários oficiais dos municípios brasileiros, foram selecionadas para esse estudo todas as capitais brasileiras, além de outras grandes cidades do país, <a href="https://github.com/bezerraescossia/imd-data-science/blob/main/municipios.txt" rel="external nofollow noopener" target="_blank">veja aqui a lista de municípios</a> contemplados nesse estudo.</p> <p>O grande desafio dessa primeira fase é automatizar a coleta dos diários oficiais, já que cada prefeitura tem seu próprio portal. Um caminho seria realizar a raspagem dos dados (<em>webscraping</em>) em cada site municipal, entretanto, isso demandaria muito tempo. Nessa perspectiva, para facilitar esse processo, o projeto querido diário já disponibiliza uma API destinada a raspagem dessas informações. A API não possui, ainda, compatibilidade com todos os municípios, entretanto, a maioria das grandes cidades já fazem parte de seu acervo.</p> <p>Para a utilização da API do querido diário foi necessário, primeiramente, a criação de um ambiente de desenvolvimento. Para isso, clonou-se o <a href="https://github.com/okfn-brasil/querido-diario" rel="external nofollow noopener" target="_blank">repositório do projeto</a>, e acessando o diretório pelo terminal, executou-se os comandos abaixo:</p> <pre>$ python3 -m venv .venv<br>$ source .venv/bin/activate<br>$ pip install -r data_collection/requirements-dev.txt<br>$ pre-commit install</pre> <p>Desse modo, foi criado o ambiente de desenvolvimento através do virtual-env, ativado e instalado todos os requisitos contidos no arquivo requirements-dev.txt.</p> <p>Após a criação do ambiente virtual e sua ativação, foi executado a aplicação de <em>webscraping</em> para cada município listado. Para isso, foi preciso dirigir-se para o diretório data_collection e executar o seguinte comando:</p> <pre>$ scrapy crawl uf_municipio -a start_date=2020-01-01</pre> <p>Substituindo-se o “uf” pela unidade federativa relativa, da mesma forma com “município”. Por exemplo, para o município de Natal/RN utilizaríamos a seguinte linha de código:</p> <pre>$ scrapy crawl rn_natal -a start_date=2020-01-01</pre> <p>Observe ainda que na pesquisa, filtrou-se a data de início da raspagem, ou seja, foi coletado o diário oficial de todos os municípios da lista, desde o dia 01 de Janeiro de 2020 até a data presente desse texto (01/12/2021). Realizado os <em>scrapings</em>, foram obtidos arquivos PDFs respectivos a cada diário oficial emitido contido no filtro aplicado. Entretanto, foi gerado uma pasta para cada dia, e por questão de organização, foi desejado concentrar todos os arquivos PDFs de um município em uma única pasta.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*R0IHh8MKpJSz9nHEx6F4Zw.png"></figure> <p>Para isso, foi executado o seguinte comando no diretório:</p> <pre>find . -name '*.pdf' -exec mv -t ./pdfs {} +</pre> <h3>Transformação e Estruturação dos Dados</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*J1qM75yTAH7Iy_xa34_84A.png"></figure> <p>Como visto anteriormente, o resultado obtido até então são arquivos PDFs relativos aos diários oficiais dos municípios. Entretanto, a extensão .pdf não é a ideal para ser trabalhada. A solução seria transformar os arquivos .pdf em .txt, além de gerar um arquivo .json com os metadados do .pdf.</p> <p>Para esse processo, o projeto querido diário também fornece os subsídios necessários, através da biblioteca querido-diario-toolbox. Para isso, foi necessário, primeiramente, instalar a biblioteca através do gerenciador de pacotes pip:</p> <pre>$ pip install querido-diario-toolbox</pre> <p>Auxiliado pela biblioteca do querido-diario-toolbox, os seguintes passos foram realizados:</p> <ol> <li>Transformação dos arquivos PDF para TXT e JSON;</li> <li>Seleção dos diários oficiais que possuem a citação de remédios comprovadamente não eficazes no combate ao COVID (cloroquina, ivermectina, hidroxicloroquina e azitromicina);</li> <li>Estruturação dos textos relevantes contidos nos arquivos TXT e JSON em um arquivo CSV para futura utilização como DataFrame;</li> <li>Substituição da coluna com os códigos dos municípios e estados do IBGE por seus respectivos nomes;</li> <li>Remoção dos textos que não possuem as palavras “COVID”, “CORONA” ou “PANDEMIA”, removendo assim boa parte de textos que apesar de possuirem a citação aos remédios não explicitam sua relação ao covid;</li> <li>Criação de coluna com os CNPJs das empresas citadas nos textos, quando houver.</li> </ol> <blockquote>Confira os códigos desenvolvidos no <a href="https://github.com/bezerraescossia/imd-data-science/blob/main/transformation.ipynb" rel="external nofollow noopener" target="_blank">repositório do GitHub</a> e o <a href="https://github.com/bezerraescossia/imd-data-science/blob/main/data/log.csv" rel="external nofollow noopener" target="_blank">arquivo CSV</a> resultante</blockquote> <h3>Análise Exploratória de Dados</h3> <p>Na análise exploratória de dados, algumas perguntas foram levantadas para conduzir essa etapa. Abaixo, é possível verificar essas perguntas e também suas respectivas respostas:</p> <p>Quantas vezes cada remédio não eficaz foi citado nos diários oficiais?</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7vErOOANQmfdrX4vDZYb6w.png"></figure> <p>Quantas vezes cada município citou um remédio não eficaz?</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SKO5bAHUv0uzCdwyw81MDQ.png"></figure> <p>Quais empresas foram mais citadas?</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ksdE2wjSa7pHo1U0VH0KnQ.png"></figure> <blockquote>Todos os códigos utilizados na análise exploratória podem ser vistas <a href="https://github.com/bezerraescossia/imd-data-science/blob/main/eda.ipynb" rel="external nofollow noopener" target="_blank">aqui</a>.</blockquote> <h3>Preparação dos Dados para Machine Learning</h3> <p>No contexto das informações apresentadas em nosso <em>dataset</em>, não se pode inferir que uma citação a um remédio não eficaz indique necessariamente a sua utilização ao combate do COVID-19, muitas vezes esses remédios são citados em diários oficiais, inclusive, para a crítica de seu uso. Além disso, o objetivo principal dessa pesquisa é encontrar comprovação do gasto de dinheiro público com esses remédios na finalidade de combater o covid-19 e não somente a sua utilização em determinado município. A fim de solucionar esse problema, foi proposto a categorização dos textos em 3 classes de acordo com sua relevância para o estudo:</p> <ol> <li>Texto não comprova a compra</li> <li>Texto sugere a aquisição dos medicamentos</li> <li>Texto sugere a aquisição dos medicamentos por dispensa de licitação</li> </ol> <p>É importante notar, o motivo de se dividir a compra dos medicamentos nos itens 2 e 3. O item 3 indica que uma empresa X foi contratada por um município Y para fornecer os medicamentos sem a realização de um processo licitatório, onde ocorre um pregão de preços e etc. Esse tipo de conduta é perigoso e talvez mereça uma atenção melhor e uma investigação mais profunda.</p> <p>Para a concretização dessa solução, será utilizado algoritmos de <em>machine learning</em>. Mas, antes disso, é necessário realizar a preparação de nosso <em>dataset</em>.</p> <p>A solução proposta baseia-se no conceito de <em>bag of words</em>, que converte uma coleção de strings em uma matriz e realiza sua contagem. Por exemplo:</p> <pre>&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer<br>&gt;&gt;&gt; corpus = [<br>...     'This is the first document.',<br>...     'This document is the second document.',<br>...     'And this is the third one.',<br>...     'Is this the first document?',<br>... ]<br>&gt;&gt;&gt; vectorizer = CountVectorizer()<br>&gt;&gt;&gt; X = vectorizer.fit_transform(corpus)<br>&gt;&gt;&gt; vectorizer.get_feature_names_out()<br>array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',<br>       'this'], ...)<br>&gt;&gt;&gt; print(X.toarray())<br>[[0 1 1 1 0 0 1 0 1]<br> [0 2 0 1 0 1 1 0 1]<br> [1 0 0 1 1 0 1 1 1]<br> [0 1 1 1 0 0 1 0 1]]</pre> <p>Observando a matriz gerada, como resultado, fica fácil de compreender essa função. Observe que temos 4 linhas, que indicam as 4 frases da variável <em>corpus</em>, cada coluna representa uma palavra que segue a ordem [‘and’, ‘document’, ‘first’, ‘is’, ‘one’, ‘second’, ‘the’, ‘third’, ‘this’]. A primeira linha da matriz indica que a primeira frase da variável <em>corpus</em> (‘This is the first document.’) possui:</p> <ul> <li>0 palavras ‘and’</li> <li>1 palavra ‘document’</li> <li>1 palavra ‘first’</li> <li>1 palavra ‘is’</li> <li>0 palavras ‘one’</li> <li>0 palavras ‘second’</li> <li>1 palavra ‘the’</li> <li>0 palavras ‘third’</li> <li>1 palavra ‘this’</li> </ul> <p>da mesma forma ocorre para a segunda frase que é a segunda coluna da matriz e assim por diante.</p> <p>Portanto, para a preparação dos dados na utilização dos algoritmos de <em>machine learning</em> não será necessária a criação de <em>features</em>, uma vez que utilizaremos o conceito de <em>bag of words</em>. Precisamos, entretanto, classificar manualmente essas mensagens, para que o modelo possa ser treinado.</p> <p>O primeiro passo foi selecionar somente as <em>features</em> necessárias para o aprendizado de maquina, do nosso <a href="https://github.com/bezerraescossia/imd-data-science/blob/main/data/log.csv" rel="external nofollow noopener" target="_blank"><em>dataset</em></a> foi selecionado somente a serie “texto”.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/944/1*0ygTTUt5_Cah5OtXBzz0EA.jpeg"></figure> <p>Por fim, foi adicionada a coluna “<em>label</em>” que recebe a categoria daquela mensagem. Como já explicado, por se tratar de um modelo supervisionado, a “rotulação” quanto a relevância da mensagem foi manualmente imputada, a partir da leitura de cada mensagem e classificação dela, por fim chegamos ao arquivo preparado para o treinamento do modelo.</p> <blockquote>Confira o <a href="https://github.com/bezerraescossia/imd-data-science/blob/main/ml_preparation.py" rel="external nofollow noopener" target="_blank">código desenvolvido</a> e o <a href="https://github.com/bezerraescossia/imd-data-science/blob/main/data/ml.csv" rel="external nofollow noopener" target="_blank">arquivo CSV</a> gerado</blockquote> <h3>Machine Learning na Classificação dos Textos</h3> <p>Carregado o <em>dataset</em> preparado na seção anterior, precisamos treinar o modelo. Mas antes, visando melhorar a interpretação do algoritmo, vamos definir os <em>stop words </em>que serão palavras excluídas no processo de “vetorização” das palavras (normalmente palavras sem significado profundo, comuns da língua). Para essa finalidade, foi utilizado a biblioteca nltk:</p> <pre>import nltk</pre> <pre>nltk.download('stopwords')</pre> <pre>from nltk.corpus import stopwords</pre> <pre>stop = stopwords.words('portuguese')</pre> <p>Confira os stop words da língua portuguesa de acordo com a biblioteca nltk:</p> <blockquote>‘de’, ‘a’, ‘o’, ‘que’, ‘e’, ‘é’, ‘do’, ‘da’, ‘em’, ‘um’, ‘para’, ‘com’, ‘não’, ‘uma’, ‘os’, ‘no’, ‘se’, ‘na’, ‘por’, ‘mais’, ‘as’, ‘dos’, ‘como’, ‘mas’, ‘ao’, ‘ele’, ‘das’, ‘à’, ‘seu’, ‘sua’, ‘ou’, ‘quando’, ‘muito’, ‘nos’, ‘já’, ‘eu’, ‘também’, ‘só’, ‘pelo’, ‘pela’, ‘até’, ‘isso’, ‘ela’, ‘entre’, ‘depois’, ‘sem’, ‘mesmo’, ‘aos’, ‘seus’, ‘quem’, ‘nas’, ‘me’, ‘esse’, ‘eles’, ‘você’, ‘essa’, ‘num’, ‘nem’, ‘suas’, ‘meu’, ‘às’, ‘minha’, ‘numa’, ‘pelos’, ‘elas’, ‘qual’, ‘nós’, ‘lhe’, ‘deles’, ‘essas’, ‘esses’, ‘pelas’, ‘este’, ‘dele’, ‘tu’, ‘te’, ‘vocês’, ‘vos’, ‘lhes’, ‘meus’, ‘minhas’, ‘teu’, ‘tua’, ‘teus’, ‘tuas’, ‘nosso’, ‘nossa’, ‘nossos’, ‘nossas’, ‘dela’, ‘delas’, ‘esta’, ‘estes’, ‘estas’, ‘aquele’, ‘aquela’, ‘aqueles’, ‘aquelas’, ‘isto’, ‘aquilo’, ‘estou’, ‘está’, ‘estamos’, ‘estão’, ‘estive’, ‘esteve’, ‘estivemos’, ‘estiveram’, ‘estava’, ‘estávamos’, ‘estavam’, ‘estivera’, ‘estivéramos’, ‘esteja’, ‘estejamos’, ‘estejam’, ‘estivesse’, ‘estivéssemos’, ‘estivessem’, ‘estiver’, ‘estivermos’, ‘estiverem’, ‘hei’, ‘há’, ‘havemos’, ‘hão’, ‘houve’, ‘houvemos’, ‘houveram’, ‘houvera’, ‘houvéramos’, ‘haja’, ‘hajamos’, ‘hajam’, ‘houvesse’, ‘houvéssemos’, ‘houvessem’, ‘houver’, ‘houvermos’, ‘houverem’, ‘houverei’, ‘houverá’, ‘houveremos’, ‘houverão’, ‘houveria’, ‘houveríamos’, ‘houveriam’, ‘sou’, ‘somos’, ‘são’, ‘era’, ‘éramos’, ‘eram’, ‘fui’, ‘foi’, ‘fomos’, ‘foram’, ‘fora’, ‘fôramos’, ‘seja’, ‘sejamos’, ‘sejam’, ‘fosse’, ‘fôssemos’, ‘fossem’, ‘for’, ‘formos’, ‘forem’, ‘serei’, ‘será’, ‘seremos’, ‘serão’, ‘seria’, ‘seríamos’, ‘seriam’, ‘tenho’, ‘tem’, ‘temos’, ‘tém’, ‘tinha’, ‘tínhamos’, ‘tinham’, ‘tive’, ‘teve’, ‘tivemos’, ‘tiveram’, ‘tivera’, ‘tivéramos’, ‘tenha’, ‘tenhamos’, ‘tenham’, ‘tivesse’, ‘tivéssemos’, ‘tivessem’, ‘tiver’, ‘tivermos’, ‘tiverem’, ‘terei’, ‘terá’, ‘teremos’, ‘terão’, ‘teria’, ‘teríamos’, ‘teriam’.</blockquote> <p>Com isso, agora podemos realizar a “vetorização” das palavras, separar nosso <em>dataset</em> em treino e teste, definir uma baseline, testar os modelos, alterar parâmetros e enfim obter o modelo final do projeto. Para testar a acurácia dos resultados, foi utilizada a estratégia da estratificação KFold (leia esse <a href="https://medium.com/data-hackers/como-avaliar-seu-modelo-de-classifica%C3%A7%C3%A3o-34e6f6011108" rel="external nofollow noopener" target="_blank">artigo</a> para compreender melhor). Abaixo está o resultado das acurácias de algumas funções do scikit-learn que foram testadas, em que o primeiro valor é a acurácia média e entre colchetes está o intervalo de acurácia.</p> <ul> <li> <strong>Modelo Árvore de Decisão:</strong> 96.48% [92.02% ~ 100.00%]</li> <li> <strong>Modelo SVC:</strong> 90.36% [81.26% ~ 99.45%]</li> <li> <strong>Modelo Regressão Logística</strong>: 94.33% [83.48% ~ 100.00%]</li> <li> <strong>Modelo Random Forest:</strong> 94.31% [89.88% ~ 98.74%]</li> <li> <strong>Modelo LinearSVC:</strong> 95.20% [85.52% ~ 100.00%]</li> <li> <strong>Modelo SGDC:</strong> 92.56% [82.45% ~ 100.00%]</li> </ul> <p>Portanto, devido a uma melhor acurácia média e menor desvio padrão, o modelo de <em>machine learning </em>utilizado para a previsão do conteúdo de texto presente no diário oficial foi o modelo Árvore de Decisão.</p> <blockquote>Visualize com mais detalhe o código escrito dessa etapa no <a href="https://github.com/bezerraescossia/imd-data-science/blob/main/ml.ipynb" rel="external nofollow noopener" target="_blank">link</a>.</blockquote> <h3>Conclusão</h3> <p>Como visto, o objetivo desse trabalho foi indicar possíveis compras dos remédios comprovadamente ineficazes no combate ao Covid-19 pelas prefeituras.</p> <p>Esse objetivo foi devidamente alcançado, uma vez que os algoritmos de machine learning permitiram uma boa acurácia do resultado na classificação dos textos contidos nos diários oficiais.</p> <p>Vale salientar que o mesmo pipeline utilizado nesse contexto, poderia ser extrapolado para outras finalidades na fiscalização de órgãos públicos, beneficiando a transparência dessas entidades com a população.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ebb776c39fda" width="1" height="1" alt=""></p> </body></html>